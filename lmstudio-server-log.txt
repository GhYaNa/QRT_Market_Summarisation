[2023-11-04 22:49:50.004] [INFO] [LM STUDIO SERVER] Server logs are ENABLED.
[2023-11-04 22:49:50.011] [INFO] [LM STUDIO SERVER] Success! HTTP server listening on port 1234.
[2023-11-04 22:49:50.012] [INFO] [LM STUDIO SERVER] Supported endpoints:
[2023-11-04 22:49:50.012] [INFO] [LM STUDIO SERVER] ->	GET  http://localhost:1234/v1/models
[2023-11-04 22:49:50.012] [INFO] [LM STUDIO SERVER] ->	POST http://localhost:1234/v1/chat/completions
[2023-11-04 22:49:50.013] [INFO] [LM STUDIO SERVER] ->	POST http://localhost:1234/v1/completions
[2023-11-04 22:49:50.013] [INFO] [LM STUDIO SERVER] Logs are saved into C:\tmp\lmstudio-server-log.txt
[2023-11-04 22:50:06.101] [INFO] Processing queued request...
[2023-11-04 22:50:06.101] [INFO] Received POST request to /v1/chat/completions with body: {
  "model": "local-model",
  "messages": [
    {
      "role": "user",
      "content": " What is 1 + 1?"
    }
  ]
}
[2023-11-04 22:50:06.102] [INFO] [LM STUDIO SERVER] Context Overflow Policy is: Rolling Window
[2023-11-04 22:50:06.102] [INFO] Provided inference configuration: {
  "n_threads": 4,
  "n_predict": -1,
  "top_k": 40,
  "top_p": 0.95,
  "temp": 0.8,
  "repeat_penalty": 1.1,
  "input_prefix": "[INST]",
  "input_suffix": "[/INST]",
  "antiprompt": [
    "[INST]"
  ],
  "pre_prompt": "",
  "pre_prompt_suffix": "\n<</SYS>>\n\n[/INST]",
  "pre_prompt_prefix": "[INST]<<SYS>>\n",
  "seed": -1,
  "tfs_z": 1,
  "typical_p": 1,
  "repeat_last_n": 64,
  "frequency_penalty": 0,
  "presence_penalty": 0,
  "n_keep": 0,
  "logit_bias": {},
  "mirostat": 0,
  "mirostat_tau": 5,
  "mirostat_eta": 0.1,
  "memory_f16": true,
  "multiline_input": false,
  "penalize_nl": true
}
[2023-11-04 22:50:11.878] [INFO] Accumulated 1 tokens:  
[2023-11-04 22:50:12.211] [INFO] Accumulated 2 tokens:   The
[2023-11-04 22:50:12.540] [INFO] Accumulated 3 tokens:   The answer
[2023-11-04 22:50:13.096] [INFO] Accumulated 4 tokens:   The answer to
[2023-11-04 22:50:13.707] [INFO] Accumulated 5 tokens:   The answer to 
[2023-11-04 22:50:14.117] [INFO] Accumulated 6 tokens:   The answer to 1
[2023-11-04 22:50:14.549] [INFO] Accumulated 7 tokens:   The answer to 1 +
[2023-11-04 22:50:15.079] [INFO] Accumulated 8 tokens:   The answer to 1 + 
[2023-11-04 22:50:15.495] [INFO] Accumulated 9 tokens:   The answer to 1 + 1
[2023-11-04 22:50:15.933] [INFO] Accumulated 10 tokens:   The answer to 1 + 1 is
[2023-11-04 22:50:16.506] [INFO] Accumulated 11 tokens:   The answer to 1 + 1 is 
[2023-11-04 22:50:16.885] [INFO] Accumulated 12 tokens:   The answer to 1 + 1 is 2
[2023-11-04 22:50:17.404] [INFO] Accumulated 13 tokens:   The answer to 1 + 1 is 2.
[2023-11-04 22:50:17.928] [INFO] Accumulated 14 tokens:   The answer to 1 + 1 is 2. nobody
[2023-11-04 22:50:18.391] [INFO] Accumulated 15 tokens:   The answer to 1 + 1 is 2. nobody.
[2023-11-04 22:50:18.790] [INFO] Generated prediction: {
  "id": "chatcmpl-zc0ow76l4ic8jyasv6uv83",
  "object": "chat.completion",
  "created": 1699138206,
  "model": "C:\\Users\\galmu\\.cache\\lm-studio\\models\\TheBloke\\Llama-2-7B-Chat-GGUF\\llama-2-7b-chat.Q2_K.gguf",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "  The answer to 1 + 1 is 2. nobody."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 15,
    "total_tokens": 15
  }
}
[2023-11-05 04:03:27.856] [INFO] [LM STUDIO SERVER] Stopping server...
[2023-11-05 04:03:27.876] [INFO] [LM STUDIO SERVER] Server stopped.
